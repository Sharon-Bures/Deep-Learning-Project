{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script layers-util.exe is installed in 'C:\\Users\\carag\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting torch_geometric\n",
      "  Obtaining dependency information for torch_geometric from https://files.pythonhosted.org/packages/97/f0/66ad3a5263aa16efb534aaf4e7da23ffc28c84efbbd720b0c5ec174f6242/torch_geometric-2.5.3-py3-none-any.whl.metadata\n",
      "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
      "     ---------------------------------------- 0.0/64.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/64.2 kB ? eta -:--:--\n",
      "     ------------------------ ------------- 41.0/64.2 kB 960.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 64.2/64.2 kB 691.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (3.1)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: torch in c:\\users\\carag\\appdata\\roaming\\python\\python311\\site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\carag\\appdata\\roaming\\python\\python311\\site-packages (0.16.1)\n",
      "Requirement already satisfied: torchsummary in c:\\users\\carag\\appdata\\roaming\\python\\python311\\site-packages (1.5.1)\n",
      "Collecting layers\n",
      "  Downloading layers-0.1.5.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from torch_geometric) (4.65.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch_geometric) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch_geometric) (1.11.1)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch_geometric) (2023.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from torch_geometric) (3.8.5)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in c:\\programdata\\anaconda3\\lib\\site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from torch_geometric) (1.3.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\carag\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: PyYaml in c:\\programdata\\anaconda3\\lib\\site-packages (from layers) (6.0)\n",
      "Collecting bashutils (from layers)\n",
      "  Downloading Bashutils-0.0.4.tar.gz (4.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\carag\\appdata\\roaming\\python\\python311\\site-packages (from aiohttp->torch_geometric) (23.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch_geometric) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torch_geometric) (2023.11.17)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->torch_geometric) (2.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
      "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.1/1.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.2/1.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.3/1.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.4/1.1 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.6/1.1 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.7/1.1 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 0.8/1.1 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.0/1.1 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.1/1.1 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 2.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: layers, bashutils\n",
      "  Building wheel for layers (setup.py): started\n",
      "  Building wheel for layers (setup.py): finished with status 'done'\n",
      "  Created wheel for layers: filename=layers-0.1.5-py3-none-any.whl size=5388 sha256=4799672dba2d966d9fd8df0e5215592bbdebb6863c1ea6df12b6b35b704da7cd\n",
      "  Stored in directory: c:\\users\\carag\\appdata\\local\\pip\\cache\\wheels\\36\\2f\\17\\808a8b9187247bb31ffb720b3a8ba0b8a90bd766160f148740\n",
      "  Building wheel for bashutils (setup.py): started\n",
      "  Building wheel for bashutils (setup.py): finished with status 'done'\n",
      "  Created wheel for bashutils: filename=Bashutils-0.0.4-py3-none-any.whl size=5484 sha256=b1415282d0784d6f8eb30f6d98ab4e732474d53590bc84f537a36b8d1d3793ad\n",
      "  Stored in directory: c:\\users\\carag\\appdata\\local\\pip\\cache\\wheels\\a8\\90\\52\\eec0445722a0f747ad367ebb29872952557bf84d9db948328a\n",
      "Successfully built layers bashutils\n",
      "Installing collected packages: bashutils, layers, torch_geometric\n",
      "Successfully installed bashutils-0.0.4 layers-0.1.5 torch_geometric-2.5.3\n"
     ]
    }
   ],
   "source": [
    "! pip install torch_geometric networkx matplotlib torch torchvision torch-geometric torchsummary layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch-scatter\n",
      "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
      "     ---------------------------------------- 0.0/108.0 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/108.0 kB ? eta -:--:--\n",
      "     ------------------------------------ - 102.4/108.0 kB 1.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 108.0/108.0 kB 1.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch-sparse\n",
      "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
      "     ---------------------------------------- 0.0/210.0 kB ? eta -:--:--\n",
      "     ----------- ---------------------------- 61.4/210.0 kB ? eta -:--:--\n",
      "     ------------------------------- ------ 174.1/210.0 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 210.0/210.0 kB 3.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch-cluster\n",
      "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
      "     ---------------------------------------- 0.0/54.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 54.5/54.5 kB 1.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch-spline-conv\n",
      "  Downloading torch_spline_conv-1.2.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch-sparse) (1.11.1)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from scipy->torch-sparse) (1.24.3)\n",
      "Building wheels for collected packages: torch-scatter, torch-sparse, torch-cluster, torch-spline-conv\n",
      "  Building wheel for torch-scatter (setup.py): started\n",
      "  Building wheel for torch-scatter (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch-scatter\n",
      "  Building wheel for torch-sparse (setup.py): started\n",
      "  Building wheel for torch-sparse (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch-sparse\n",
      "  Building wheel for torch-cluster (setup.py): started\n",
      "  Building wheel for torch-cluster (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch-cluster\n",
      "  Building wheel for torch-spline-conv (setup.py): started\n",
      "  Building wheel for torch-spline-conv (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch-spline-conv\n",
      "Failed to build torch-scatter torch-sparse torch-cluster torch-spline-conv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [33 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\torch_scatter\n",
      "  copying torch_scatter\\placeholder.py -> build\\lib.win-amd64-cpython-311\\torch_scatter\n",
      "  copying torch_scatter\\scatter.py -> build\\lib.win-amd64-cpython-311\\torch_scatter\n",
      "  copying torch_scatter\\segment_coo.py -> build\\lib.win-amd64-cpython-311\\torch_scatter\n",
      "  copying torch_scatter\\segment_csr.py -> build\\lib.win-amd64-cpython-311\\torch_scatter\n",
      "  copying torch_scatter\\testing.py -> build\\lib.win-amd64-cpython-311\\torch_scatter\n",
      "  copying torch_scatter\\utils.py -> build\\lib.win-amd64-cpython-311\\torch_scatter\n",
      "  copying torch_scatter\\__init__.py -> build\\lib.win-amd64-cpython-311\\torch_scatter\n",
      "  creating build\\lib.win-amd64-cpython-311\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\logsumexp.py -> build\\lib.win-amd64-cpython-311\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\softmax.py -> build\\lib.win-amd64-cpython-311\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\std.py -> build\\lib.win-amd64-cpython-311\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\__init__.py -> build\\lib.win-amd64-cpython-311\\torch_scatter\\composite\n",
      "  running egg_info\n",
      "  writing torch_scatter.egg-info\\PKG-INFO\n",
      "  writing dependency_links to torch_scatter.egg-info\\dependency_links.txt\n",
      "  writing requirements to torch_scatter.egg-info\\requires.txt\n",
      "  writing top-level names to torch_scatter.egg-info\\top_level.txt\n",
      "  reading manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*' found under directory 'test'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
      "  running build_ext\n",
      "  C:\\Users\\carag\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\cpp_extension.py:383: UserWarning: Error checking compiler version for cl: [WinError 2] Das System kann die angegebene Datei nicht finden\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_scatter._scatter_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for torch-scatter\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [58 lines of output]\n",
      "  running bdist_wheel\n",
      "  C:\\Users\\carag\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\cpp_extension.py:502: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "    warnings.warn(msg.format('we could not find ninja.'))\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\add.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\bandwidth.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\cat.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\coalesce.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\convert.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\diag.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\eye.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\index_select.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\masked_select.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\matmul.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\metis.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\mul.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\narrow.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\permute.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\reduce.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\rw.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\saint.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\sample.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\select.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\spadd.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\spmm.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\spspmm.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\storage.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\tensor.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\testing.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\transpose.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\typing.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\utils.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  copying torch_sparse\\__init__.py -> build\\lib.win-amd64-cpython-311\\torch_sparse\n",
      "  running egg_info\n",
      "  writing torch_sparse.egg-info\\PKG-INFO\n",
      "  writing dependency_links to torch_sparse.egg-info\\dependency_links.txt\n",
      "  writing requirements to torch_sparse.egg-info\\requires.txt\n",
      "  writing top-level names to torch_sparse.egg-info\\top_level.txt\n",
      "  reading manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\css'\n",
      "  warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\html'\n",
      "  warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\tests'\n",
      "  warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\examples'\n",
      "  warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\benchmark'\n",
      "  warning: no previously-included files matching '*' found under directory 'test'\n",
      "  warning: no previously-included files matching '*' found under directory 'benchmark'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
      "  running build_ext\n",
      "  C:\\Users\\carag\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\cpp_extension.py:383: UserWarning: Error checking compiler version for cl: [WinError 2] Das System kann die angegebene Datei nicht finden\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_sparse._convert_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for torch-sparse\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [32 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  copying torch_cluster\\fps.py -> build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  copying torch_cluster\\graclus.py -> build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  copying torch_cluster\\grid.py -> build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  copying torch_cluster\\knn.py -> build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  copying torch_cluster\\nearest.py -> build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  copying torch_cluster\\radius.py -> build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  copying torch_cluster\\rw.py -> build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  copying torch_cluster\\sampler.py -> build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  copying torch_cluster\\testing.py -> build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  copying torch_cluster\\typing.py -> build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  copying torch_cluster\\__init__.py -> build\\lib.win-amd64-cpython-311\\torch_cluster\n",
      "  running egg_info\n",
      "  writing torch_cluster.egg-info\\PKG-INFO\n",
      "  writing dependency_links to torch_cluster.egg-info\\dependency_links.txt\n",
      "  writing requirements to torch_cluster.egg-info\\requires.txt\n",
      "  writing top-level names to torch_cluster.egg-info\\top_level.txt\n",
      "  reading manifest file 'torch_cluster.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*' found under directory 'test'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'torch_cluster.egg-info\\SOURCES.txt'\n",
      "  running build_ext\n",
      "  C:\\Users\\carag\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\cpp_extension.py:383: UserWarning: Error checking compiler version for cl: [WinError 2] Das System kann die angegebene Datei nicht finden\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_cluster._fps_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for torch-cluster\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [26 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\torch_spline_conv\n",
      "  copying torch_spline_conv\\basis.py -> build\\lib.win-amd64-cpython-311\\torch_spline_conv\n",
      "  copying torch_spline_conv\\conv.py -> build\\lib.win-amd64-cpython-311\\torch_spline_conv\n",
      "  copying torch_spline_conv\\testing.py -> build\\lib.win-amd64-cpython-311\\torch_spline_conv\n",
      "  copying torch_spline_conv\\weighting.py -> build\\lib.win-amd64-cpython-311\\torch_spline_conv\n",
      "  copying torch_spline_conv\\__init__.py -> build\\lib.win-amd64-cpython-311\\torch_spline_conv\n",
      "  running egg_info\n",
      "  writing torch_spline_conv.egg-info\\PKG-INFO\n",
      "  writing dependency_links to torch_spline_conv.egg-info\\dependency_links.txt\n",
      "  writing requirements to torch_spline_conv.egg-info\\requires.txt\n",
      "  writing top-level names to torch_spline_conv.egg-info\\top_level.txt\n",
      "  reading manifest file 'torch_spline_conv.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*' found under directory 'test'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'torch_spline_conv.egg-info\\SOURCES.txt'\n",
      "  running build_ext\n",
      "  C:\\Users\\carag\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\cpp_extension.py:383: UserWarning: Error checking compiler version for cl: [WinError 2] Das System kann die angegebene Datei nicht finden\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_spline_conv._basis_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for torch-spline-conv\n",
      "ERROR: Could not build wheels for torch-scatter, torch-sparse, torch-cluster, torch-spline-conv, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "! pip install torch-scatter torch-sparse torch-cluster torch-spline-conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import random\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes missing 'sentiment': 578173\n"
     ]
    }
   ],
   "source": [
    "# Load the data and check how many nodes don't have the sentiment label\n",
    "# Function to load JSON\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Function to build a graph\n",
    "def build_graph(data):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add root tweet\n",
    "    root_id = data['id']\n",
    "    G.add_node(root_id, **data)\n",
    "    \n",
    "    # Recursive function to add children\n",
    "    def add_children(children, parent_id):\n",
    "        for child in children:\n",
    "            G.add_node(child['id'], **child)\n",
    "            G.add_edge(parent_id, child['id'])\n",
    "            if 'children' in child:\n",
    "                add_children(child['children'], parent_id)\n",
    "    \n",
    "    if 'children' in data:\n",
    "        add_children(data['children'], root_id)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Function to count nodes missing sentiment\n",
    "def count_nodes_missing_sentiment(graph):\n",
    "    total_missing_sentiment = 0\n",
    "    for node, attrs in graph.nodes(data=True):\n",
    "        if 'sentiment' not in attrs or attrs['sentiment'] is None:\n",
    "            total_missing_sentiment += 1\n",
    "    return total_missing_sentiment\n",
    "\n",
    "# Function to normalize node attributes\n",
    "def normalize_node_attributes(graph):\n",
    "    all_attributes = set()\n",
    "    for _, attrs in graph.nodes(data=True):\n",
    "        all_attributes.update(attrs.keys())\n",
    "    \n",
    "    for node, attrs in graph.nodes(data=True):\n",
    "        for attr in all_attributes:\n",
    "            if attr not in attrs:\n",
    "                attrs[attr] = None  # Or use another default value \n",
    "    return graph\n",
    "\n",
    "# Adjusted convert to data\n",
    "def convert_to_data(graph, label):\n",
    "    data = from_networkx(graph)\n",
    "    \n",
    "    # Ensure node features exist and are consistent\n",
    "    if not hasattr(data, 'x') or data.x is None:\n",
    "        num_nodes = data.num_nodes\n",
    "        degrees = torch.tensor([degree for _, degree in graph.degree()], dtype=torch.float).view(-1, 1)\n",
    "        clustering = torch.tensor([nx.clustering(graph, node) for node in graph.nodes], dtype=torch.float).view(-1, 1)\n",
    "        data.x = torch.cat([degrees, clustering], dim=1)\n",
    "    \n",
    "    # Ensure all required attributes are included\n",
    "    if not hasattr(data, 'y') or data.y is None:\n",
    "        data.y = torch.tensor([label] * data.num_nodes, dtype=torch.long)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Path to the dataset\n",
    "path = 'nx_network_data/nx_network_data'  # Change to individual path\n",
    "\n",
    "# Function to create a dictionary of JSON files\n",
    "def create_json_dict(base_path):\n",
    "    json_dict = {}\n",
    "    \n",
    "    for label in ['politifact_fake', 'politifact_real']:\n",
    "        folder_path = os.path.join(base_path, label)\n",
    "        files = os.listdir(folder_path)\n",
    "        files = [f for f in files if f.endswith('.json')]\n",
    "        for file in files:  \n",
    "            json_dict[os.path.join(label, file)] = 'fake' if label == 'politifact_fake' else 'real'\n",
    "    \n",
    "    return json_dict\n",
    "\n",
    "json_dict = create_json_dict(path)\n",
    "\n",
    "full_dataset = []\n",
    "total_missing_sentiment = 0\n",
    "\n",
    "# Loop to create a mega dataset\n",
    "for dataset in list(json_dict.keys()):\n",
    "    file = load_json(os.path.join(path, dataset))  # Correctly join paths\n",
    "    graph = build_graph(file)\n",
    "    \n",
    "    # Count nodes missing sentiment\n",
    "    missing_sentiment_count = count_nodes_missing_sentiment(graph)\n",
    "    total_missing_sentiment += missing_sentiment_count\n",
    "    \n",
    "    graph = normalize_node_attributes(graph)\n",
    "    label = 1 if json_dict[dataset] == 'fake' else 0\n",
    "    data = convert_to_data(graph, label)\n",
    "    \n",
    "    if data is not None:\n",
    "        full_dataset.append(data)\n",
    "\n",
    "print(f\"Total nodes missing 'sentiment': {total_missing_sentiment}\")\n",
    "\n",
    "# Check and clean up the dataset\n",
    "cleaned_dataset = [data for data in full_dataset if data.y is not None]\n",
    "\n",
    "# Splitting the dataset\n",
    "fake_news_data = [data for data in cleaned_dataset if data.y[0].item() == 1]\n",
    "real_news_data = [data for data in cleaned_dataset if data.y[0].item() == 0]\n",
    "\n",
    "balanced_full_dataset = fake_news_data + random.choices(fake_news_data, k=len(real_news_data) - len(fake_news_data)) + real_news_data\n",
    "random.shuffle(balanced_full_dataset)\n",
    "\n",
    "train_size = int(0.7 * len(balanced_full_dataset))\n",
    "val_size = int(0.15 * len(balanced_full_dataset))\n",
    "test_size = len(balanced_full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(balanced_full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(0.5)  # added dropout layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data and remove the sentiment attribute\n",
    "# Function to load JSON\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Function to build a graph\n",
    "def build_graph(data):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add root tweet\n",
    "    root_id = data['id']\n",
    "    if 'sentiment' in data:\n",
    "        del data['sentiment']  # Remove sentiment attribute\n",
    "    G.add_node(root_id, **data)\n",
    "    \n",
    "    # Recursive function to add children\n",
    "    def add_children(children, parent_id):\n",
    "        for child in children:\n",
    "            if 'sentiment' in child:\n",
    "                del child['sentiment']  # Remove sentiment attribute\n",
    "            G.add_node(child['id'], **child)\n",
    "            G.add_edge(parent_id, child['id'])\n",
    "            if 'children' in child:\n",
    "                add_children(child['children'], child['id'])\n",
    "    \n",
    "    if 'children' in data:\n",
    "        add_children(data['children'], root_id)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Function to normalize node attributes\n",
    "def normalize_node_attributes(graph):\n",
    "    all_attributes = set()\n",
    "    for _, attrs in graph.nodes(data=True):\n",
    "        all_attributes.update(attrs.keys())\n",
    "    \n",
    "    for node, attrs in graph.nodes(data=True):\n",
    "        for attr in all_attributes:\n",
    "            if attr not in attrs:\n",
    "                attrs[attr] = None  # Or use another default value \n",
    "    return graph\n",
    "\n",
    "\n",
    "# Adjusted convert to data\n",
    "def convert_to_data(graph, label):\n",
    "    data = from_networkx(graph)\n",
    "    \n",
    "    # Ensure node features exist and are consistent\n",
    "    if not hasattr(data, 'x') or data.x is None:\n",
    "        num_nodes = data.num_nodes\n",
    "        degrees = torch.tensor([degree for _, degree in graph.degree()], dtype=torch.float).view(-1, 1)\n",
    "        clustering = torch.tensor([nx.clustering(graph, node) for node in graph.nodes], dtype=torch.float).view(-1, 1)\n",
    "        data.x = torch.cat([degrees, clustering], dim=1)\n",
    "    \n",
    "    # Ensure all required attributes are included\n",
    "    if not hasattr(data, 'y') or data.y is None:\n",
    "        data.y = torch.tensor([label] * data.num_nodes, dtype=torch.long)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Path to the dataset\n",
    "path = 'nx_network_data/nx_network_data'    # Change to individual path\n",
    "\n",
    "# Function to create a dictionary of JSON files\n",
    "def create_json_dict(base_path):\n",
    "    json_dict = {}\n",
    "    \n",
    "    for label in ['politifact_fake', 'politifact_real']:\n",
    "        folder_path = os.path.join(base_path, label)\n",
    "        files = os.listdir(folder_path)\n",
    "        files = [f for f in files if f.endswith('.json')]\n",
    "        for file in files:  \n",
    "            json_dict[os.path.join(label, file)] = 'fake' if label == 'politifact_fake' else 'real'\n",
    "    \n",
    "    return json_dict\n",
    "\n",
    "json_dict = create_json_dict(path)\n",
    "\n",
    "full_dataset = []\n",
    "\n",
    "# Loop to create a mega dataset\n",
    "for dataset in list(json_dict.keys()):\n",
    "    file = load_json(os.path.join(path, dataset))  # Correctly join paths\n",
    "    graph = build_graph(file)\n",
    "    graph = normalize_node_attributes(graph)\n",
    "    label = 1 if json_dict[dataset] == 'fake' else 0\n",
    "    data = convert_to_data(graph, label)\n",
    "    \n",
    "    if data is not None:\n",
    "        full_dataset.append(data)\n",
    "\n",
    "# Check and clean up the dataset\n",
    "cleaned_dataset = [data for data in full_dataset if data.y is not None]\n",
    "\n",
    "\n",
    "# Splitting the dataset\n",
    "fake_news_data = [data for data in cleaned_dataset if data.y[0].item() == 1]\n",
    "real_news_data = [data for data in cleaned_dataset if data.y[0].item() == 0]\n",
    "\n",
    "balanced_full_dataset = fake_news_data + random.choices(fake_news_data, k=len(real_news_data) - len(fake_news_data)) + real_news_data\n",
    "random.shuffle(balanced_full_dataset)\n",
    "\n",
    "train_size = int(0.7 * len(balanced_full_dataset))\n",
    "val_size = int(0.15 * len(balanced_full_dataset))\n",
    "test_size = len(balanced_full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(balanced_full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(0.5)  # added dropout layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the EarlyStopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=25, min_delta=0.0001, path='checkpoint.pt'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            if val_loss < self.best_loss:\n",
    "                self.save_checkpoint(val_loss, model)\n",
    "                self.best_loss = val_loss\n",
    "                self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f'Validation loss decreased ({self.best_loss:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "\n",
    "# Define the training function\n",
    "def train_model(epochs, model, optimizer, criterion, train_loader, val_loader, early_stopper, device, checkpoint_path):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for data in train_loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.x, data.edge_index)\n",
    "            loss = criterion(output, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                data = data.to(device)\n",
    "                output = model(data.x, data.edge_index)\n",
    "                loss = criterion(output, data.y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        \n",
    "        print(f'Epoch: {epoch+1}, Training Loss: {train_loss / len(train_loader):.4f}, Validation Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "        early_stopper(val_loss, model)\n",
    "        if early_stopper.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    return train_losses, val_losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(2, 16)\n",
      "  (conv2): GCNConv(16, 2)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Epoch: 1, Training Loss: 278.7524, Validation Loss: 350.9149\n",
      "Validation loss decreased (16492.999742 --> 16492.999742).  Saving model ...\n",
      "Epoch: 2, Training Loss: 248.6027, Validation Loss: 324.6030\n",
      "Validation loss decreased (16492.999742 --> 15256.342283).  Saving model ...\n",
      "Epoch: 3, Training Loss: 245.1599, Validation Loss: 296.9014\n",
      "Validation loss decreased (15256.342283 --> 13954.365019).  Saving model ...\n",
      "Epoch: 4, Training Loss: 219.3559, Validation Loss: 269.2669\n",
      "Validation loss decreased (13954.365019 --> 12655.545672).  Saving model ...\n",
      "Epoch: 5, Training Loss: 218.9396, Validation Loss: 239.9239\n",
      "Validation loss decreased (12655.545672 --> 11276.421448).  Saving model ...\n",
      "Epoch: 6, Training Loss: 160.5592, Validation Loss: 215.2856\n",
      "Validation loss decreased (11276.421448 --> 10118.424992).  Saving model ...\n",
      "Epoch: 7, Training Loss: 178.9537, Validation Loss: 188.3588\n",
      "Validation loss decreased (10118.424992 --> 8852.862313).  Saving model ...\n",
      "Epoch: 8, Training Loss: 135.9363, Validation Loss: 161.9658\n",
      "Validation loss decreased (8852.862313 --> 7612.394366).  Saving model ...\n",
      "Epoch: 9, Training Loss: 119.0169, Validation Loss: 136.2000\n",
      "Validation loss decreased (7612.394366 --> 6401.398277).  Saving model ...\n",
      "Epoch: 10, Training Loss: 101.5342, Validation Loss: 116.6255\n",
      "Validation loss decreased (6401.398277 --> 5481.399310).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, optimizer, and loss function\n",
    "input_dimensions = 2  # Assuming two input features\n",
    "hidden_dimensions = 16\n",
    "output_dimensions = 2\n",
    "gcn_model = GCN(in_channels=input_dimensions, hidden_channels=hidden_dimensions, out_channels=output_dimensions)\n",
    "optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopper = EarlyStopping(patience=25, min_delta=0.0001, path='gnn_checkpoint.pt')\n",
    "\n",
    "# Move model to device\n",
    "gcn_model.to(device)\n",
    "print(gcn_model)\n",
    "\n",
    "# Train the GNN model\n",
    "epochs = 10\n",
    "train_losses_gcn, val_losses_gcn = train_model(epochs, gcn_model, optimizer, criterion, train_loader, val_loader, early_stopper, device, 'gnn_checkpoint.pt')\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.plot(train_losses_gcn, label='Training Loss GCN')\n",
    "plt.plot(val_losses_gcn, label='Validation Loss GCN')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss GCN')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5438202,
     "sourceId": 9023838,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
